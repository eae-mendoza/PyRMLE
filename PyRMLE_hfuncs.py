import matplotlib.pyplot as plt
import scipy.optimize as scop
import random
import numpy as np
import time
from sklearn.cluster import KMeans

def ransample_bivar(n,pi,mu,sigma):
    """ Function used to generate \betas from a normal mixture """
    x=np.zeros((n))
    y=np.zeros((n))
    # This step determines from which distribution to sample from based on the 
    # probabilities given
    k=np.random.choice(len(pi),n,p=pi,replace=True)
    for i in range(0,len(k)):
        x[i],y[i]=np.random.multivariate_normal(mu[k[i]],sigma[k[i]],1).T
    return x,y

def shape(result):
    f = result.f
    dim = result.dim
    m = result.grid.ks()[0]
    if dim  == 2:
        return f.reshape(m,m)
    else:
        return f.reshape(m,m,m)
    
def dt_mtx(interval,shift):
    x1=interval*shift[0]
    x2=interval*shift[1]
    coord_mtx=np.zeros((len(x1),len(x2),2))
    for i in range(0,len(x1)):
        coord_mtx[:,i]=np.array((x1,full(len(x1),x2[i]))).T
    return coord_mtx

def sim_sample2d(n,x_params=None,beta_pi=None,beta_mu=None,beta_cov=None):
    # Checks if user declares any parameters to use.
    if not x_params:
        x_params = [-2,2]
    else:
        x_params = x_params
    if not beta_pi:
        beta_pi = [0.5,0.5]
    else:
        beta_pi = beta_pi
    if not beta_mu:
        beta_mu = [[-1.5,-1.5],[1.5,1.5]]
    else:
        beta_mu = beta_mu
    if not beta_cov:
        beta_cov = [[[1, 0], [0, 1]],[[1, 0], [0, 1]]]
    else:
        beta_cov = beta_cov
    # Sample the regressor X_1 from a uniform distribution
    x1=np.random.uniform(x_params[0],x_params[1],n).T
    # X_0 is generated by creating a column of ones
    x0=np.repeat(1,n)
    # Create an array of regressors
    xs = np.c_[x0,x1]
    # Sample the random coefficients from a bivariate mixture 
    # Using the function ransample_bivar
    b0,b1=ransample_bivar(n,beta_pi,beta_mu,beta_cov)
    b={'col1':b0,'col2':b1}
    bs = np.c_[b0,b1]
    # Computes for B'*X
    bx=bs*xs
    # Generate the response variable Y 
    y=np.array([sum(x) for x in bx])
    xy_sample_sim=np.c_[xs,y]
    return xy_sample_sim

def sim_sample3d(n,x_params=None,beta_pi=None,beta_mu=None,beta_cov=None):
    # Checks if user declares any parameters to use.
    if not x_params:
        x_params = [-2,2]
    else:
        x_params = x_params
    if not beta_pi:
        beta_pi = [1,0,0]
    else:
        beta_pi = beta_pi
    if not beta_mu:
        beta_mu = [[2,2,2],[2,2,2],[2,2,2]]
    else:
        beta_mu = beta_mu
    if not beta_cov:
        beta_cov = [[[1, 0, 0], [0,1,0], [0,0,1]],[[1, 0, 0], [0,1,0], [0,0,1]],[[1, 0, 0], [0,1,0], [0,0,1]]]
    else:
        beta_cov = beta_cov
    # Sample the regressors X_1, and X_2 iid from a uniform distribution.
    x1=np.random.uniform(x_params[0],x_params[1],n).T
    x2=np.random.uniform(x_params[0],x_params[1],n).T
    x0=np.repeat(1,n)
    #creating the np.array of Xs
    xs = np.c_[x0,x1,x2]
    # Sample the random coefficients from a gaussian mixture distribution using the
    # function ransample()
    b0,b1,b2=ransample(n,beta_pi,beta_mu,beta_cov)
    bs = np.c_[b0,b1,b2]
    bx=bs*xs
    # Generate the response variable Y
    y=np.array([sum(x) for x in bx])
    xy_sample_sim=np.c_[xs,y]
    return xy_sample_sim

def sim_sample(n,d,x_params=None,beta_pi=None,beta_mu=None,beta_cov=None):
    if d == 2:
        return sim_sample2d(n,x_params=None,beta_pi=None,beta_mu=None,beta_cov=None)
    else:
        return sim_sample3d(n,x_params=None,beta_pi=None,beta_mu=None,beta_cov=None)

def filt(start,end,step,array):
    """ This function filters out \beta values that exceed the range of
    the grid. 
    """
    t=array[:,0]>=start
    array=array[t]
    t1=array[:,0]<=end-step
    array=array[t1]
    t2=array[:,1]>=start
    array=array[t2]
    t3=array[:,1]<=end-step
    array=array[t3]
    return array

def dist_xy(p1,p2):
    return np.sqrt((p1[0]-p2[0])**2+(p1[1]-p2[1])**2)

def filt2(array,slope):
    n=len(array)
    if (slope[1]<=0 and slope[0]>=0) or (slope[1]>=0 and slope[0]<=0):
        return array[1:n]
    else:
        return array[1:n]

def which_Bi(point,Bi_grid):
    if len(b1s[b1s<point[0]])>0: 
        i=len(b1s[b1s<point[0]])-1
    else:
        i=0
    if len(b0s[b0s<point[1]])>0:
        j=len(b0s[b0s<point[1]])-1
    else:
        j=0
    return Bi_grid[i][j]

def likelihood_wrapper(f0,sample,b1s,b0s,start,end):
    fs=np.array([f_yx_test(f0,i[0],i[1],b1s,b0s,start,end) for i in sample])
    return -sum(np.log(fs))


def which_ij(point,slope,interval):
    """This function takes an intersection point as an argument and determines the
    2-dimensional index of the intersection point based on the grid. """
    b0s=interval
    b1s=interval
    # 
    if (slope[1]<=0 and slope[0]>=0) or (slope[1]>=0 and slope[0]<=0):
        if len(b1s[b1s<point[0]])>0: 
            i=len(b1s[b1s<point[0]])-1
        else:
            i=0
        if len(b0s[b0s<point[1]])>0:
            j=len(b0s[b0s<point[1]])-1
        else:
            j=0
        return i,j
    else:
        if len(b1s[b1s<point[0]])>0: 
            i=len(b1s[b1s<point[0]])-1
        else:
            i=0
        if len(b0s[b0s<=point[1]])>0:
            j=len(b0s[b0s<=point[1]])-1
        else:
            j=0
        return i,j

def index_conv(ijs,k):
    """ This function converts the indices i and j, obtained from the function
    which_ij() into the index of the 1 x m dimensional array form of \hat{f_\beta}
    """
    val=np.array([x[1]*np.sqrt(k)+x[0] for x in ijs])
    return val

def get_intervals(xi,yi,grid):
    """ This function computes the length of intersection of the line parametrized by the sample 
    observations X and Y with the grid (i.e. returns all the lengths of the line-segments generated
    by passing a line through the grid)
    """
    start = grid.start
    end = grid.end
    step = grid.step
    b0s=grid.interval
    b1s=grid.interval
    # Computes for intersection points for different cases
    if xi[1]!= 0 and xi[0]!=0:
        b0=(yi-b1s*xi[1])/xi[0]
        b1=(yi-b0s*xi[0])/xi[1]
    elif xi[1]==0:
        b0=[yi]*len(b1s)
        b1=b1s
    else:
        b0=b0s
        b1=[yi]*len(b1s)
    b1based=np.c_[b1s,b0]
    b0based=np.c_[b1,b0s]
    b1b0=np.r_[b1based,b0based]
    b1b0=b1b0[np.argsort(b1b0[:,0])]
    # Filters only unique intersections
    b1b0=np.unique(b1b0,axis=0)
    # Filters intersection points within the grid
    new_b1b0=filt(start,end,step,b1b0)
    # Computes the length of the line intervals
    intervals=[np.linalg.norm(new_b1b0[i]-new_b1b0[i+1]) for i in range(0,len(new_b1b0)-1)]
    return intervals

def get_intersections(xi,yi,grid):
    """ This function computes for the intersection points on the grid.
    """
    start = grid.start
    end = grid.end
    step = grid.step
    b0s=grid.interval
    b1s=grid.interval
    if xi[1]!= 0 and xi[0]!=0:
        b0=(yi-b1s*xi[1])/xi[0]
        b1=(yi-b0s*xi[0])/xi[1]
    elif xi[1]==0:
        b0=[yi]*len(b1s)
        b1=b1s
    else:
        b0=b0s
        b1=[yi]*len(b1s)
    b1based=np.c_[b1s,b0]
    b0based=np.c_[b1,b0s]
    b1b0=np.r_[b1based,b0based]
    b1b0=b1b0[np.argsort(b1b0[:,0])]
    b1b0=np.unique(b1b0,axis=0)
    new_b1b0=filt(start,end,step,b1b0)
    reduced_b1b0=filt2(new_b1b0,xi)
    return reduced_b1b0

def transmatrix_2d(sample,grid):
    """ This function is used to generate the transformation matrix for the 2-dimensional
    application.
    
    The output is the class tmatrix which is used as an argument in the wrapper function
    rmle().
    """
    xy_sample = sample.scaled_sample.copy()
    L=np.zeros((len(xy_sample),grid.numgridpoints()))
    for n in range(0,len(xy_sample)):
        intervals=get_intervals(xy_sample[n],xy_sample[n][2],grid)
        intersection=get_intersections(xy_sample[n],xy_sample[n][2],grid)
        indices=index_conv([which_ij(p,xy_sample[n],grid.interval) for p in intersection],grid.numgridpoints())
        indices=list(map(int,indices))
        for i in range(0,len(indices)):
            L[n][indices[i]]=intervals[i]
    L = L[~np.all(L==0, axis=1)]
    return tmatrix(Tmat=L,grid=grid,scaled_sample=xy_sample,sample=sample.sample)


def likelihood(f,n,L_mat_long):
    """ This function is the log-likelihood functional without a regularization 
    term.
    """
    import numpy as np
    L_mat=L_mat_long.reshape(n,len(f))
    f[f<0]=10**-6
    val=np.log(np.dot(L_mat,f))
    return -sum(val)/n

def norm_sq(f,alpha,n,L_mat_long,step):
    """ This function is the log-likelihood functional with the squared L2 norm 
    of \hat{f_\beta} as the regularization term.
    """
    import numpy as np
    L_mat=L_mat_long.reshape(n,len(f))
    f[f<0]=10**-6
    val=np.log(np.dot(L_mat,f))
    return -sum(val)/n+ alpha*step**2*sum(f**2)

def sobolev(f,alpha,n,L_mat_long,step):
    """ This function is the log-likelihood functional with the sobolev norm 
    for H1 as the regularization term.
    """
    import numpy as np
    L_mat=L_mat_long.reshape(n,len(f))
    f[f<0]=10**-6
    val=-np.sum(np.log(np.dot(L_mat,f)))/n
    penal=alpha*step**2*sum(f**2)+alpha*step**2*norm_fprime(f,step)
    return val + penal

def entropy(f,alpha,n,L_mat_long,step):
    """ This function is the log-likelihood functional with the entropy of \hat{f_\beta} 
    as the regularization term.
    """
    import numpy as np
    L_mat=L_mat_long.reshape(n,len(f))
    f[f<0]=10**-6
    val=np.log(np.dot(L_mat,f))
    return -sum(val)/n + alpha*step**2*sum(f*np.log(f))

def tot_deriv(f,step):
    """ This function comutes the total derivative of f"""
    import numpy as np
    f=f.reshape(int(np.sqrt(len(f))),int(np.sqrt(len(f))))
    fgrad=np.gradient(f,step)
    return np.ravel((np.sqrt(fgrad[0]**2+fgrad[1]**2)))

def norm_fprime(f,step):
    """ This function computes d/df (||f||^{2}_{2}). """
    import numpy as np
    f=f.reshape(int(np.sqrt(len(f))),int(np.sqrt(len(f))))
    fgrad=np.gradient(f,step)
    return sum(np.ravel((fgrad[0]**2+fgrad[1]**2)))

def second_deriv(f,step):
    """ This function computes the Laplacian of f."""
    import numpy as np
    f=f.reshape(int(np.sqrt(len(f))),int(np.sqrt(len(f))))
    fgrad0=np.ravel(np.gradient(np.gradient(f,step)[0],step)[0])
    fgrad1=np.ravel(np.gradient(np.gradient(f,step)[1],step)[1])
    return fgrad0+fgrad1

def jac_likelihood(f,n,L_mat_long):
    """ This function computes the jacobian of the regularization
    functional without any penalty term.
    """
    import numpy as np
    L_mat=L_mat_long.reshape(n,len(f))
    denom=np.dot(L_mat,f)
    val=L_mat.T/denom
    return -val.T.sum(axis=0)/n


def jac_norm_sq(f,alpha,n,L_mat_long,step):
    """ This function computes the jacobian of the regularization 
    functional with the squared L2 norm penalty.
    """
    import numpy as np
    L_mat=L_mat_long.reshape(n,len(f))
    denom=np.dot(L_mat,f)
    val=L_mat.T/denom
    return -val.T.sum(axis=0)/n+alpha*step**2*2*f

"""def jac_sobolev2(f,alpha,n):
    denom=np.dot(L_mat,f)
    val=L_mat.T/denom
    return -val.T.sum(axis=0)/n+alpha*step**2*2*f+2*alpha*step**2*second_deriv(f)"""

def jac_sobolev(f,alpha,n,L_mat_long,step):
    """ This function computes the jacobian of the regularization
    functional with the H1 penalty.
    """
    import numpy as np
    L_mat=L_mat_long.reshape(n,len(f))
    denom=np.dot(L_mat,f)
    val=L_mat.T/denom
    return -val.T.sum(axis=0)/n+alpha*step**2*2*f-2*alpha*step**2*second_deriv(f,step)

def jac_entropy(f,alpha,n,L_mat_long,step):
    """ This function computes the jacobian of the regularization
    functional with the entropy penalty.
    """
    import numpy as np
    L_mat=L_mat_long.reshape(n,len(f))
    denom=np.dot(L_mat,f)
    val=L_mat.T/denom
    return -val.T.sum(axis=0)/n+alpha*step**2*(np.log(f)+1)

def ise(f1,f2,step_size):
    val=np.linalg.norm(f1-f2)**2
    return len(f1)**-1*step_size**2*val

def mise(f1,f2,step_size):
    val=np.linalg.norm(f1-f2)**2
    return step_size**2*val

def sq_l2(f1,f2):
    val = np.linalg.norm(f1-f2)**2
    return val

def prop_n(r,q,i):
    """ This function computes for the bounding function used in
    the implementation of Lepkii's balancing principle for selecting
    \alpha. 
    """
    return 8*r**((1-i)/q)

def get_int(x):
    if any(x<0):
        return next(i for i in x if i<=0)
    else:
        return x[-1]
    
def updt(total, progress):

    """
    Displays or updates a console progress bar.
    Source: https://stackoverflow.com/questions/3160699/python-progress-bar
    
    """
    barLength, status = 20, ""
    progress = float(progress) / float(total)
    if progress >= 1.:
        progress, status = 1, "\r\n"
    block = int(round(barLength * progress))
    text = "\r[{}] {:.0f}% {}".format(
        "#" * block + "-" * (barLength - block), round(progress * 100, 0),
        status)
    sys.stdout.write(text)
    sys.stdout.flush()
    
def select_cluster(y,kmean_obj,l2s):
    """ This function is used to select which cluster to not discard when implementing
    the shifting algorithm.
    """
    arr_l2 = np.array(l2s)
    clus1 = len(np.where(y==1)[0])
    clus0 = len(np.where(y==0)[0])
    if clus1 > clus0:
        return np.where(y==1)[0],1
    elif clus0 > clus1:
        return np.where(y==0)[0],0
    else:
        clus_center1 = kmean_obj.cluster_centers_[1][0]
        clus_center0 = kmean_obj.cluster_centers_[1][0]
        if clus_center1 < clus_center0:
            return np.where(y==1)[0],1
        else: 
            return np.where(y==0)[0],0

def shift_scale_loc(grid,loc):
    """ This fuction takes a location which is an array then backtransforms the
    shift and scaling applied to it in the process of estimation.
    """
    loc_copy = loc.copy()
    if grid.dim == 2:
        loc_copy[0] = loc_copy[0]/grid.b0-grid.shifts[0]
        loc_copy[1] = loc_copy[1]/grid.b1-grid.shifts[1]
    else:
        loc_copy[0] = loc_copy[0]/grid.b0-grid.shifts[0]
        loc_copy[1] = loc_copy[1]/grid.b1-grid.shifts[1]
        loc_copy[2] = loc_copy[2]/grid.b2-grid.shifts[2]
    return loc_copy


def index_finder(diffs):
    """ This function is used to identify the index of the \alpha value that
    satisfies the Lepskii balancing principle criteria.
    """
    i=0
    for d in diffs:
        val = np.sum(d[d<0])
        i+=1
        if val != 0:
            break
    return int(i-1)

def prop_n(r,q,i):
    return 8*r**((1-i)/q)

def alpha_vals(a,n):
    """ This function generates the values of \alpha to be used for cross validation. """
    itrs=[]
    for i in range(0,n):
        itrs.append(a*0.9**i)
    itrs.reverse()
    return itrs

def alpha_lep(n,sample_size,r):
    """ This function generates the values of \alpha to be used for Lepskii's method"""
    a=1/(2*sample_size)*np.log(sample_size)/np.sqrt(sample_size)
    itrs=[]
    for i in range(0,n):
        itrs.append(a*r**i)
    itrs=[i for i in itrs if i <=2]
    return itrs

def sample_shuffle(sample):
    """ This function applies a  random shuffle on the subsamples used in the process of
    cross validation.
    """
    n=len(sample)
    indices=np.arange(0,n)
    random.shuffle(indices)
    i=0
    new_sample=[]
    for i in indices:
        new_sample.append(sample[i])
    return new_sample

def cv_index(n,k):
    """ Generates the indices used to slice the sample into the k-folds for cross
    validation.
    """
    n_k = int(np.ceil(n/k))
    if n%k != 0:
        indices=np.arange(0,n,n_k)[0:-1]
    else:
        indices=np.arange(0,n,n_k)
    return indices

def cv_loss(a,alphas,n,k,progress,total,functional,initial_guess,trans_matrix,trans_matrix_long,step_size,jacobian,hessian_method,constraints,tolerance,max_iter,bound):
    """ This fucntion is used to compute for the loss function in cross-validation. """
    indices=cv_index(n,k)
    f_n=scop.minimize(functional,initial_guess,args=(a,n,trans_matrix_long,step_size),method='trust-constr',jac=jacobian,hess=hessian_method,constraints=[constraints],tol=tolerance,options={'verbose': False,'maxiter': max_iter},bounds=bound)
    fs=[]
    trans_matrix_slices=[]
    inv_trans_matrix_slices=[]
    trans_matrix_slices.append(trans_matrix[indices[1]:])
    trans_matrix_slices.append(trans_matrix[:indices[-1]])
    inv_trans_matrix_slices.append(trans_matrix[:indices[1]])
    inv_trans_matrix_slices.append(trans_matrix[indices[-1]:])
    j=progress
    loss=0
    for i in range(1,len(indices)-1):
        trans_matrix_slices.append(np.concatenate((trans_matrix[:indices[i]],trans_matrix[indices[i+1]:])))
        inv_trans_matrix_slices.append(trans_matrix[indices[i]:indices[i+1]])
    for t,i in zip(trans_matrix_slices,inv_trans_matrix_slices):
        trans_matrix_slice_long = np.ravel(t)
        inv_trans_matrix_slice_long = np.ravel(i)
        n=len(t)
        n_i = len(i)
        val_f=scop.minimize(functional,initial_guess,args=(a,n,trans_matrix_slice_long,step_size),method='trust-constr',jac=jacobian,hess=hessian_method,constraints=[constraints],tol=tolerance,options={'verbose': False,'maxiter': max_iter},bounds=bound).x
        fs.append(val_f)
        loss+=likelihood_l(val_f,inv_trans_matrix_slice_long,n_i)
        j+=1
        updt(total,j)
    return f_n.x,loss/k,j

def quarter_selector(alphas,n,k,progress,total,functional,initial_guess,trans_matrix,trans_matrix_long,step_size,jacobian,hessian_method,constraints,tolerance,max_iter,bound):
    """ This function is the implementation of the modified k-fold cross validation method. """
    alphas=alphas
    len_a =len(alphas)
    quart_a = int(np.ceil(len_a)*0.75)
    p,q=random.randint(0,quart_a-1),randint(quart_a,len_a-1)
    a_p=alphas[p]
    a_q=alphas[q]
    val1=cv_loss(a_p,alphas,n,k,progress,total,functional,initial_guess,trans_matrix,trans_matrix_long,step_size,jacobian,hessian_method,constraints,tolerance,max_iter,bound)
    val2=cv_loss(a_q,alphas,n,k,progress,total,functional,initial_guess,trans_matrix,trans_matrix_long,step_size,jacobian,hessian_method,constraints,tolerance,max_iter,bound)
    alphas=np.delete(alphas,[p,q])
    if val1[1] < val2[1]:
        alphas=alphas[:quart_a-2]
        return val1[0],alphas,val1[1],val1[2],a_p
    else:
        alphas=alphas[quart_a-1:len_a-2]
        return val2[0],alphas,val2[1],val2[2],a_q
    
def rmle_2d(functional,alpha,tmat,shift=None,k=None,jacobian=None,initial_guess=None,hessian_method=None,constraints=None,tolerance=None,max_iter=None,bounds=None):
    """ This function is a sub-function of rmle(). This is a wrapper function for the SciPy Optimize minimize()
    function. It has 3 essential arguments which are: {functional, alha, tmat}. 
    
    Essential/Positional arguments:
    
    functional - the choice of regularization functional
    
    alpha - the choice of the alpha parameter which can be a constant or a string matching 'cv' or 'lepskii'
    
    tmat - the class object tmatrix generated by the the transmatrix() function. This contains the 
    transformation matrix used in the evaluation of the functional and its jacobian.
    
    Optional arguments:
    
    shift - Unused for rmle_2d()
    
    k - an integer value that specifies the number of folds to be used in k-fold cross validation
     
    jacobian - accepts a function as an argument or string type, for more options the please refer to the
    documentation of SciPy's minimize function for more details. In the case of this module, the jacobian of 
    functional's explicit form is provided in the form of a function. We advise to keep this argument to its
    default status.
    
    intiial_guess - an array argument that matches the dimensions of \hat{f_\bbeta}.
     
    hessian_method -  the default is the '2-point' method implemented in the SciPy library. The user can 
    provide the other hessian methods avaiable to the scipy optimize minimize() function.
    
    constraints - the linear constraint for the optimization problem, namely: ||\hat{f_\bbeta}||_L1 = 1 i.e.
    the estimate \hat{f_\bbeta} should integrate to 1.
    
    tolerance - a floating point argument that sets the tolerance value for the optimization problem.
    
    max_iter - an integer that sets the maximum number of iterations.
    
    bounds - sets the bounds for the values of the estimate, the default value is a non-negativity constraint.
    """ 
    n=tmat.n()
    m=tmat.m()
    step_size = tmat.grid.step
    trans_matrix_long = np.ravel(tmat.Tmat)
    trans_matrix = tmat.Tmat
    if not initial_guess:
        initial_guess = np.zeros(m)+0.000001
    else:
        initial_guess = initial_guess
    if not jacobian:
        if functional == likelihood:
            jacobian = jac_likelihood
        elif functional == norm_sq:
            jacobian = jac_norm_sq
        elif functional == sobolev:
            jacobian = jac_sobolev
        elif functional == entropy:
            jacobian =  jac_entropy
    else:
        jacobian = jacobian
    if not hessian_method:
        hessian_method = '2-point'
    else:
        hessian_method = hessian_method
    if not constraints:
        linear_constraint=scop.LinearConstraint([step_size**2]*len(initial_guess),[1],[1])
        constraints = linear_constraint
    else:
        constraints = constraints
    if not tolerance:
        tolerance = 1e-6
    else:
        tolerance = tolerance
    if not max_iter:
        max_iter = 100
    else: 
        max_iter = max_iter
    if not bounds:
        bound = scop.Bounds(0,np.inf)
    else:
        bound = bounds
    if not k:
        k = 20
    else:
        k = k
    if alpha=='Lepskii' or alpha=='lepskii':
        r=1.2
        alphas=alpha_lep(70,n,r)
        reconstructions=[]
        total = len(alphas)
        i_range=np.arange(0,len(alphas))
        j=0
        times=[]
        for a in alphas:
            rec = scop.minimize(functional,initial_guess,args=(a,n,trans_matrix_long,step_size),method='trust-constr',jac=jacobian,hess=hessian_method,constraints=[constraints],tol=tolerance,options={'verbose': False,'maxiter': max_iter},bounds=bound)
            reconstructions.append(rec.x)
            j+=1
            updt(total,j)
        approx_errs = []
        for i in range(1,len(reconstructions)):
            jj=0
            approx_error = []
            while jj < i:
                approx_error.append(np.linalg.norm(reconstructions[i]-reconstructions[jj]))
                jj+=1
            approx_errs.append(approx_error)
        prop_errors=prop_n(r,2,i_range)
        diffs=[]
        for p in approx_errs:
            n_p = int(len(p))
            diffs.append(np.array(prop_errors[:n_p])-np.array(p))
        index=index_finder(diffs)
        return RMLEResult(f=reconstructions[index],alpha=alphas[index],alpmth='Lepskii',T=tmat,details=None)
    if alpha=='cv' or alpha == 'CV':
        alphas=alpha_vals(step_size*3,30)
        lhood=[]
        reconstructions=[]
        alpha_list=[]
        trans_matrix=sample_shuffle(trans_matrix)
        j=0
        total = np.ceil(len(alphas)*np.log(2/len(alphas))/np.log(0.5))
        while len(alphas)>=2:
            val=quarter_selector(alphas,n,k,j,total,functional,initial_guess,trans_matrix,trans_matrix_long,step_size,jacobian,hessian_method,constraints,tolerance,max_iter,bound)
            lhood.append(val[2])
            reconstructions.append(val[0])
            j=val[3]
            alphas=val[1]
            alpha_list.append(val[4])
        for a in alphas:
            val=cv_loss(a,alphas,n,k,j,total,functional,initial_guess,trans_matrix,trans_matrix_long,step_size,jacobian,hessian_method,constraints,tolerance,max_iter,bound)
            lhood.append(val[1])
            reconstructions.append(val[0])
            alpha_list.append(a)
            j=val[2]
        index=np.int(np.where(lhood==np.min(lhood))[0])
        updt(total,total)
        return RMLEResult(f=reconstructions[index],alpha=alpha_list[index],alpmth='CV',T=tmat,details=None)
    else:
        result = scop.minimize(functional,initial_guess,args=(alpha,n,trans_matrix_long,step_size),method='trust-constr',jac=jacobian,hess=hessian_method,constraints=[constraints],tol=tolerance,options={'verbose': 1,'maxiter': max_iter},bounds=bound)
        return RMLEResult(f=result.x,alpha=alpha,alpmth='User',T=tmat,details=None)

def edge_check(p,start,end):
   """ This function checks if the coordinates of a point lies at the edge of a grid. It returns a list of 
   boolean values. """
   check=[]
   for i in p:
       check.append((i-end)==0 or (i-start)==0)
    return check
  
def outer_edge_check(p,start,end):
  """ This function checks if a specific coordinate is at the edge of the grid."""
  return (p-end)==0 or (p-start)==0

 
def get_index(point,interval,k):
    """ This function maps an intersection point to its index on the grid i.e. in which box of 
    the grid the point lies in. """
    interval_round=np.array([round(i,8) for i in interval])
    x=[]
    for p in point:
        x.append(max(np.where(interval_round>=p)[0][0]-1,0))
    index = x[0] + x[1]*k**(1/3) + x[2]*k**(2/3)
    return np.ceil(index)

 def point_check(point,start,end):
    """ This function checks if the coordinates of a point lies at the edge of a grid. It returns a list of 
    boolean values. """
    check=0
    for i in point:
        if i == start or i == end:
            check+=1
    return check

def vertex_check(point,interval):
    """ This function checks if a point has coordinates that lies on the grid, but are not on outer-edges. """
    val = 0
    for p in point:
        if p in interval:
            val+=1
    return val

def get_box_index(point,start,end,step_size,interval,ks):
    """ This function takes a point and returns all the grid boxes (its index) that it lies in. An
    intersection point can lie in more than one box in the grid e.g. if a point lies on a vertex of the
    grid, it lies in eight grid boxes. 
    """
    point=np.array(point)
    if point_check(point,start,end) ==3: #case for outer-vertex
        indices = np.array([get_index(point,interval,ks[2])],dtype=int) 
        return list(indices)
    elif ((point_check(point,start,end) == 2) and  vertex_check(point,interval) == 3): 
        #case for corner-outer-edge vertex
        if (abs(point[1])-abs(start)==0 and abs(point[2])-abs(start)==0) or (abs(point[1])-abs(end)==0 and abs(point[2])-abs(end)==0):
            indices = np.array([get_index(point,interval,ks[2])] * 2,dtype=int)
            adjustments = np.array([0,1],dtype=int)
            return list(indices+adjustments)
        elif (abs(point[0])-abs(start)==0 and abs(point[2])-abs(start)==0) or (abs(point[0])-abs(end)==0 and abs(point[2])-abs(end)==0):
            indices = np.array([get_index(point,interval,ks[2])] * 2,dtype=int)
            adjustments = np.array([0,ks[0]],dtype=int)
            return list(indices+adjustments) 
        elif (abs(point[0])-abs(start)==0 and abs(point[1])-abs(start)==0) or (abs(point[0])-abs(end)==0 and abs(point[1])-abs(end)==0):
            indices = np.array([get_index(point,interval,ks[2])] * 2,dtype=int)
            adjustments = np.array([0,ks[1]],dtype=int)
            return list(indices+adjustments)         
    elif ((sum((abs(point)-abs(start))==0) == 1 or sum((abs(point)-abs(end))==0) == 1) and vertex_check(point,interval) == 3): 
        #case for side-outer-edge vertex
        #cases for different axes
        if (abs(point[1])-abs(start)==0) or (abs(point[1])-abs(end)==0):
            indices = np.array([get_index(point,interval,ks[2])] * 4,dtype=int)
            adjustments = np.array([0,1,ks[1],ks[1]+1],dtype=int)
            return list(indices+adjustments)
        elif (abs(point[0])-abs(start)==0) or (abs(point[0])-abs(end)==0):
            indices = np.array([get_index(point,interval,ks[2])] * 4,dtype=int)
            adjustments = np.array([0,ks[0],ks[1],ks[1]+ks[0]],dtype=int)
            return list(indices+adjustments)
        elif abs(point[2])-abs(start)==0:
            indices = np.array([get_index(point,interval,ks[2])] * 4,dtype=int)
            adjustments = np.array([0,1,ks[0],ks[0]+1],dtype=int)
            return list(indices+adjustments)
    elif vertex_check(point,interval) == 3: #case for inner-vertex
        indices = np.array([get_index(point,interval,ks[2])] * 8,dtype=int)
        adjustments = np.array([0,1,ks[0],ks[0]+1,ks[1],ks[1]+1,+ks[0]+ks[1],ks[0]+ks[1]+1],dtype=int)
        return list(indices+adjustments)
    elif sum(edge_check(point,start,end))==2: #case for outermost-edge
        indices = np.array([get_index(point,interval,ks[2])],dtype=int) 
        return list(indices)
    elif sum(edge_check(point,start,end))==1: #case for outer-edge
        if outer_edge_check(point[1],start,end): #change np.logic
            if point[0] in interval:
                indices = np.array([get_index(point,interval,ks[2])]*2,dtype=int)
                adjustments = np.array([0,1])
                return list(indices+adjustments)
            else:
                indices = np.array([get_index(point,interval,ks[2])]*2,dtype=int)
                adjustments = np.array([0,ks[1]])
                return list(indices+adjustments)                
        elif outer_edge_check(point[0],start,end):
            if point[1] in interval:
                indices = np.array([get_index(point,interval,ks[2])]*2,dtype=int)
                adjustments = np.array([0,ks[0]])
                return list(indices+adjustments)
            else:
                indices = np.array([get_index(point,interval,ks[2])]*2,dtype=int)
                adjustments = np.array([0,ks[1]])
                return list(indices+adjustments)  
        elif outer_edge_check(point[2],start,end):
            if point[0] in interval:
                indices = np.array([get_index(point,interval,ks[2])]*2,dtype=int)
                adjustments = np.array([0,1])
                return list(indices+adjustments)
            else:
                indices = np.array([get_index(point,interval,ks[2])]*2,dtype=int)
                adjustments = np.array([0,ks[0]])
                return list(indices+adjustments)  
    elif vertex_check(point,interval)==2 : #case for inner-edge #needs more cases
        if point[0] not in interval:
            indices = np.array([get_index(point,interval,ks[2])]*4,dtype=int)
            adjustments = np.array([0,ks[0],ks[1],ks[0]+ks[1]])
            return list(indices+adjustments)
        elif point[1] not in interval:
            indices = np.array([get_index(point,interval,ks[2])]*4,dtype=int)
            adjustments = np.array([0,1,ks[1],ks[1]+1])
            return list(indices+adjustments)
        elif point[2] not in interval:
            indices = np.array([get_index(point,interval,ks[2])]*4,dtype=int)
            adjustments = np.array([0,1,ks[0],ks[0]+1])
            return list(indices+adjustments)
        
        
def angle(v1,v2):
    """ This function computes the angle between two vectors. This is used to sort points in order to
    form the polygonal intersection of the plane with the grid. 
    """
    prod = np.dot(v1[0:2],v2[0:2])
    det = v1[0]*v2[1] - v1[1]*v2[0]
    temp_cos = prod/np.linalg.norm(v1[0:2])/np.linalg.norm(v2[0:2])
    if temp_cos >=0:
        cos_t = min(1,temp_cos)
    else:
        cos_t = max(-1,temp_cos)
    theta =  np.arccos(cos_t)
    if det <=0:
        return theta
    else:
        return theta + np.pi

def point_sorter(points):
    """ Unsorted points are arranged into the polygon the form based on the angles between the 
    vectors. Vectors are drawn from the center of the points.
    """
    cent = sum(points)/len(points)
    vects = cent - points
    projs = [i[0:2] for i in vects]
    sorted_points = []
    angles = []
    for i in range(len(projs)):
        angles.append(angle(projs[0], projs[i]))
    idx = np.argsort(angles)
    points = np.array(points)
    sorted_points = points[idx]
    sorted_points = sorted_points.tolist()
    return sorted_points

def area_poly(points):
    """ This function computes the area of the polygon formed by the intersection points after
    they are sorted.
    """
    area = 0.5 * abs(points[0][0] * points[-1][1] - points[0][1] * points[-1][0])
    for i in range(len(points) - 1):
        area += 0.5 * abs(points[i][0] * points[i + 1][1] - points[i][1] * points[i + 1][0])
    return area

def ransample(n,pi,mu,sigma):
    """ This function draws variables from a multivariate normal distribution. """
    x=np.zeros((n))
    y=np.zeros((n))
    z=np.zeros((n))
    k=np.random.choice(len(pi),n,p=pi,replace=True)
    for i in range(0,len(k)):
        x[i],y[i],z[i]=np.random.multivariate_normal(mu[k[i]],sigma[k[i]],1).T
    return x,y,z


def transmatrix_3d(sample,grid):
    """ This function is the sub-function of transmatrix() that is used to generate the transformation matrix
    for the 3-dimensional application of the method. Creates the class object tmatrix which is used as an
    argument to the rmle() function.
    """
    xy_sample = sample.scaled_sample.copy()
    interval=grid.interval
    L=np.zeros((len(xy_sample),grid.numgridpoints()))
    b0s=interval
    b1s=interval
    b2s=interval
    start = grid.start
    end = grid.end
    ks = grid.ks()
    step = grid.step
    for n in range(0,len(xy_sample)):
        b0_based_intersections = []
        b1_based_intersections = []
        b2_based_intersections = []
        for i in b1s:
            for j in b2s:
                b0_int = (xy_sample[n,3] - i*xy_sample[n,1] - j*xy_sample[n,2])/xy_sample[n,0]
                if b0_int >= start and b0_int <= end:
                    b0_based_intersections.append(tuple([b0_int,i,j]))
        for i in b0s:
            for j in b2s:
                b1_int = (xy_sample[n,3] - i*xy_sample[n,0] - j*xy_sample[n,2])/xy_sample[n,1]
                if b1_int >= start and b1_int <= end:
                    b1_based_intersections.append(tuple([i,b1_int,j]))
        for i in b0s:
            for j in b1s:
                b2_int = (xy_sample[n,3] - i*xy_sample[n,0] - j*xy_sample[n,1])/xy_sample[n,2]
                if b2_int >= start and b2_int <= end:
                    b2_based_intersections.append(tuple([i,j,b2_int]))
        b0_based_intersections.extend(b1_based_intersections)
        b0_based_intersections.extend(b2_based_intersections)
        intersection_points = b0_based_intersections  
        intersect_points = list(set(intersection_points))
        test_list = [[] for i in range(0,ks[2])]
        for p in intersect_points:
            indices=get_box_index(np.array(p),start,end,step,interval,ks)
            for i in indices:
                test_list[i].append(np.array(p))
        areas = [[] for i in range(0,ks[2])]
        i = 0
        for ps in test_list:
            if len(ps) > 2:
                areas[i].append(area_poly(point_sorter(ps)))
                i+=1
            else:
                areas[i].append(0)
                i+=1
        areas_flat = []
        for sublist in areas:
            for i in sublist:
                areas_flat.append(i)
        area_indices = list(np.where(np.array(areas_flat)>0)[0])
        for i in area_indices:
            L[n][i]=areas_flat[i]
    L= L[~np.all(L==0, axis=1)]
    return tmatrix(Tmat=L,grid=grid,scaled_sample=xy_sample,sample=sample.sample)

def second_deriv_3d(f,step):
    """ This function computes the Laplacian of f"""
    f=f+10e-3
    f=f.reshape(int(np.ceil(len(f)**(1/3))),int(np.ceil(len(f)**(1/3))),int(np.ceil(len(f)**(1/3))))
    fgrad0=np.ravel(np.gradient(np.gradient(f,step)[0],step)[0])
    fgrad1=np.ravel(np.gradient(np.gradient(f,step)[1],step)[1])
    fgrad2=np.ravel(np.gradient(np.gradient(f,step)[2],step)[2])
    return fgrad0+fgrad1+fgrad2

def likelihood_3d(f,n,L_mat_long):
    """ This function is the log-likelihood functional without a regularization 
    term.
    """
    import numpy as np
    L_mat=L_mat_long.reshape(n,len(f))
    f[f<0]=10**-6
    val=np.log(np.dot(L_mat,f))
    return -sum(val)/n

def sobolev_3d(f,alpha,n,L_mat_long,step):
    """ This function computes the value of the functional with the H1 penalty for the 3-d 
    implementation of the method."""
    import numpy as np
    L_mat=L_mat_long.reshape(n,len(f))
    f[f<0]=0
    f=f+10e-3
    val=np.log(np.dot(L_mat,f))
    return -sum(val)/n + alpha*step**3*sum(f**2)+alpha*step**3*norm_fprime_3d(f,step)

def norm_fprime_3d(f,step):
    """ This function computes the value of the norm of the derivative of f """
    f=f.reshape(int(np.ceil(len(f)**(1/3))),int(np.ceil(len(f)**(1/3))),int(np.ceil(len(f)**(1/3))))
    fgrad=np.gradient(f,step)
    return sum(np.ravel((fgrad[0]**2+fgrad[1]**2+fgrad[2]**2)))

def jac_sobolev_3d(f,alpha,n,L_mat_long,step):
    """ This function computes the value of the jacobian of the functional with the H1 penalty for
    the 3-d implementation of the method.
    """
    import numpy as np
    f=f+10e-3
    L_mat=L_mat_long.reshape(n,len(f))
    denom=np.dot(L_mat,f)
    val=L_mat.T/denom
    return -val.T.sum(axis=0)/n+alpha*step**3*2*f-2*alpha*step**3*second_deriv_3d(f,step)
 
def norm_sq_3d(f,alpha,n,L_mat_long,step):
    """ This function comptues the value of the functional with the squared L2 norm penalty for the 3-d 
    implementation of the method."""
    import numpy as np
    L_mat=L_mat_long.reshape(n,len(f))
    f[f<0]=10**-6
    val=np.log(np.dot(L_mat,f))
    return -sum(val)/n+ alpha*step**3*sum(f**2)

def entropy_3d(f,alpha,n,L_mat_long,step):
    """ This function comptues the value of the functional with the entropy penalty for the 3-d 
    implementation of the method."""
    L_mat=L_mat_long.reshape(n,len(f))
    f[f<0]=10**-6
    val=np.log(np.dot(L_mat,f))
    return -sum(val)/n + alpha*step**3*sum(f*np.log(f))


def jac_likelihood_3d(f,n,L_mat_long):
    """ This function computes the jacobian of the regularization
    functional without any penalty term.
    """
    L_mat=L_mat_long.reshape(n,len(f))
    denom=np.dot(L_mat,f)
    val=L_mat.T/denom
    return -val.T.sum(axis=0)/n


def jac_norm_sq_3d(f,alpha,n,L_mat_long,step):
    """ This function computes the value of the jacobian of the functional with the squared L2 norm penalty for
    the 3-d implementation of the method.
    """
    L_mat=L_mat_long.reshape(n,len(f))
    denom=np.dot(L_mat,f)
    val=L_mat.T/denom
    return -val.T.sum(axis=0)/n+alpha*step**3*2*f

def jac_entropy_3d(f,alpha,n,L_mat_long,step):
    """ This function computes the value of the jacobian of the functional with the entropy penalty for
    the 3-d implementation of the method.
    """
    L_mat=L_mat_long.reshape(n,len(f))
    f[f<0]=10**-6
    denom=np.dot(L_mat,f)
    val=L_mat.T/denom
    return -val.T.sum(axis=0)/n+alpha*step**3*(np.log(f)+1)


def rmle_3d(functional,alpha,tmat,shift=None,k=None,jacobian=None,initial_guess=None,hessian_method=None,constraints=None,tolerance=None,max_iter=None,bounds=None):
    """ This function is a sub-function of rmle(). This is a wrapper function for the SciPy Optimize minimize()
    function. It has 3 essential arguments which are: {functional, alha, tmat}. 
    
    Essential/Positional arguments:
    
    functional - the choice of regularization functional
    
    alpha - the choice of the alpha parameter which can be a constant or a string matching 'cv' or 'lepskii'
    
    tmat - the class object tmatrix generated by the the transmatrix() function. This contains the 
    transformation matrix used in the evaluation of the functional and its jacobian.
    
    Optional arguments:
    
    shift - When set to true, it implements the shifting algorithm used to prevent the instability in the 
    algorithm when all random coefficients are centralized at zero.
    
    k - an integer value that specifies the number of folds to be used in k-fold cross validation
     
    jacobian - accepts a function as an argument or string type, for more options the please refer to the
    documentation of SciPy's minimize function for more details. In the case of this module, the jacobian of 
    functional's explicit form is provided in the form of a function. We advise to keep this argument to its
    default status.
    
    intiial_guess - an array argument that matches the dimensions of \hat{f_\bbeta}.
     
    hessian_method -  the default is the '2-point' method implemented in the SciPy library. The user can 
    provide the other hessian methods avaiable to the scipy optimize minimize() function.
    
    constraints - the linear constraint for the optimization problem, namely: ||\hat{f_\bbeta}||_L1 = 1 i.e.
    the estimate \hat{f_\bbeta} should integrate to 1.
    
    tolerance - a floating point argument that sets the tolerance value for the optimization problem.
    
    max_iter - an integer that sets the maximum number of iterations.
    
    bounds - sets the bounds for the values of the estimate, the default value is a non-negativity constraint.
    """     
    n=tmat.n()
    m=tmat.m()
    trans_matrix_long = np.ravel(tmat.Tmat)
    trans_matrix = tmat.Tmat
    step_size = tmat.grid.step
    sample = tmat.sample.copy()
    if not initial_guess:
        initial_guess = np.zeros(m)+0.000001
    else:
        initial_guess = initial_guess
    if not jacobian:
        if functional == sobolev_3d:
            jacobian = jac_sobolev_3d
        elif functional == likelihood_3d:
            jacobian = jac_likelihood_3d
        elif functional == norm_sq_3d:
            jacobian = jac_norm_sq_3d
        elif functional == entropy_3d:
            jacobian = jac_entropy_3d
        else: 
            jacobian = jacobian
    if not hessian_method:
        hessian_method = '2-point'
    else:
        hessian_method = hessian_method
    if not constraints:
        linear_constraint=scop.LinearConstraint([step_size**3]*len(initial_guess),[1],[1])
        constraints = linear_constraint
    else:
        constraints = constraints
    if not tolerance:
        tolerance = 1e-16
    else:
        tolerance = tolerance
    if not max_iter:
        max_iter = 100
    else: 
        max_iter = max_iter
    if not bounds:
        bound = scop.Bounds(0,np.inf)
    else:
        bound = bounds
    if not k:
        k = 20
    else:
        k = k
    if shift == True:
        num_shifts = 10
        reconstructions = []
        shift_tmatrix_list = []
        i=0
        while i < num_shifts:
            shifts = np.array([np.random.uniform(1,2.5)/tmat.grid.b0,0,0]) + np.array(tmat.grid.shifts)
            shifted_grid = grid_obj(interval=tmat.grid.interval,scale=tmat.grid.scale, shifts = shifts, dim=tmat.grid.dim,step=step_size,start=tmat.grid.start,end=tmat.grid.end)
            shift_tmatrix = transmatrix(sample,shifted_grid)
            n=shift_tmatrix.n()
            shift_tmatrix_list.append(shift_tmatrix)
            shift_trans_matrix_long = np.ravel(shift_tmatrix.Tmat)
            rec = scop.minimize(functional,initial_guess,args=(alpha,n,shift_trans_matrix_long,step_size),method='trust-constr',jac=jacobian,hess=hessian_method,constraints=[constraints],tol=tolerance,options={'verbose': 1,'maxiter': max_iter},bounds=bound)
            reconstructions.append(rec.x)
            i+=1
        l2s = []
        for r in reconstructions:
            l2s.append(np.linalg.norm(r))
        l2_arr = np.array(l2s).reshape(-1,1)
        kmeans = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 300, n_init = 10, random_state=0)
        ys = kmeans.fit_predict(l2_arr)
        indices = select_cluster(ys,kmeans,l2s)[0]
        centroid = kmeans.cluster_centers_[select_cluster(ys,kmeans,l2s)[1]][0]
        diff_l2_arr = abs(l2_arr[indices] - centroid)
        min_index = np.where(diff_l2_arr==np.min(diff_l2_arr))[0][0]
        rec_choice = reconstructions[min_index]
        shifted_tmatrix = shift_tmatrix_list[min_index]
        return RMLEResult(f=reconstructions[min_index],alpha=alpha,alpmth='User',T=shifted_tmatrix,details=None)
    if alpha=='Lepskii' or alpha=='lepskii':
        r=1.2
        alphas=alpha_lep(70,n,r)
        reconstructions=[]
        total = len(alphas)
        i_range=np.arange(0,len(alphas))
        j=0
        times=[]
        for a in alphas:
            rec = scop.minimize(functional,initial_guess,args=(a,n,trans_matrix_long,step_size),method='trust-constr',jac=jacobian,hess=hessian_method,constraints=[constraints],tol=tolerance,options={'verbose': False,'maxiter': max_iter},bounds=bound)
            reconstructions.append(rec.x)
            j+=1
            updt(total,j)
        approx_errs = []
        for i in range(1,len(reconstructions)):
            jj=0
            approx_error = []
            while jj < i:
                approx_error.append(np.linalg.norm(reconstructions[i]-reconstructions[jj]))
                jj+=1
            approx_errs.append(approx_error)
        prop_errors=prop_n(r,2,i_range)
        diffs=[]
        for p in approx_errs:
            n_p = int(len(p))
            diffs.append(np.array(prop_errors[:n_p])-np.array(p))
        index=index_finder(diffs)
        return RMLEResult(f=reconstructions[index],alpha=alphas[index],alpmth='Lepskii',T=tmat,details=None)
    if alpha=='cv' or alpha == 'CV':
        alphas=alpha_vals(step_size*3,30)
        lhood=[]
        reconstructions=[]
        alpha_list=[]
        trans_matrix=sample_shuffle(trans_matrix)
        j=0
        total = np.ceil(len(alphas)*np.log(2/len(alphas))/np.log(0.75))
        while len(alphas)>=2:
            val=quarter_selector(alphas,n,k,j,total,functional,initial_guess,trans_matrix,trans_matrix_long,step_size,jacobian,hessian_method,constraints,tolerance,max_iter,bound)
            lhood.append(val[2])
            reconstructions.append(val[0])
            j=val[3]
            alphas=val[1]
            alpha_list.append(val[4])
        for a in alphas:
            val=cv_loss(a,alphas,n,k,j,total,functional,initial_guess,trans_matrix,trans_matrix_long,step_size,jacobian,hessian_method,constraints,tolerance,max_iter,bound)
            lhood.append(val[1])
            reconstructions.append(val[0])
            alpha_list.append(a)
            j=val[2]
        index=np.int(np.where(lhood==np.min(lhood))[0])
        updt(total,total)
        return RMLEResult(f=reconstructions[index],alpha=alpha_list[index],alpmth='CV',T=tmat,details=None)
    else:
        result = scop.minimize(functional,initial_guess,args=(alpha,n,trans_matrix_long,step_size),method='trust-constr',jac=jacobian,hess=hessian_method,constraints=[constraints],tol=tolerance,options={'verbose': 1,'maxiter': max_iter},bounds=bound)
        return RMLEResult(f=result.x,alpha=alpha,alpmth='User',T=tmat,details=None)     
    
def scale_sample(xy_sample,grid):
    """ This function is used to apply the shifts and scaling to the sample. """
    sample = xy_sample.copy()
    if grid.dim == 2:
        sample[:,2] = sample[:,2] + grid.shifts[0]*sample[:,0] + grid.shifts[1]*sample[:,1]
        sample[:,0] = sample[:,0]/grid.b0
        sample[:,1] = sample[:,1]/grid.b1 
        return sample
    else:
        sample[:,3] = sample[:,3] + grid.shifts[0]*sample[:,0] + grid.shifts[1]*sample[:,1] + grid.shifts[2]*sample[:,2]
        sample[:,0] = sample[:,0]/grid.b0
        sample[:,1] = sample[:,1]/grid.b1
        sample[:,2] = sample[:,2]/grid.b2
        return sample
    
class shft_sample_obj:
    """ Class used to store the sample and the shifts applied to it Output of the function shift_sample.""" 
    def __init__(self,sample,shifts):
        self.sample = sample
        self.shifts = shifts

class sample_obj:
    """ Class used to store the sample and the scaled sample."""
    def __init__(self,scaled_sample,sample):
        self.scaled_sample = scaled_sample
        self.sample = sample
        
def shift_sample(sample,base_shifts):
    """ This function is used to apply shifts to the sample. """
    shift_sample_obs = sample.copy()
    n = np.shape(shift_sample_obs)[1]
    shifts = [np.random.uniform(1,2.5) for i in range(0,n-1)] + np.array(base_shifts)
    for i in range(0,n-1):
        shift_sample_obs[:,n-1]=shifts[i]*shift_sample_obs[:,i]+shift_sample_obs[:,n-1]
    return shft_sample_obj(shift_sample_obs,shifts)
